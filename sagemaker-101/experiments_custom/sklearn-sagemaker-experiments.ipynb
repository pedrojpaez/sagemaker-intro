{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamline Modeling With Amazon SageMaker Studio and Amazon Experiments SDK\n",
    "\n",
    "Modeling phase is a highly iterative process in Machine Learning projects, where Data Scientists experiment with various data pre-processing and feature engineering strategies, intertwined with different model architectures, which are then trained with disparate sets of hyperparameter values. This highly iterative process, with many moving parts, can, over time, manifest into a tremendous headache in terms of keeping track of all design decisions applied in each iteration and how the training and evaluation metrics of each iteration compare to the previous versions of the model.\n",
    "\n",
    "This notebook walks you through an end-to-end example of how [Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio.html) can effectively leverage [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) to organize, track, visualize and compare our iterative work during development of a Keras model, trained to predict the age of an abalone, a sea snail, based on a set of features that describe it. While this example is specific to Keras, the same approach can be extended to other Machine Learning frameworks and algorithms. \n",
    "\n",
    "Amazon SageMaker Studio and Amazon SageMaker Experiments were unveiled at the [AWS Re:invent](https://aws.amazon.com/new/reinvent/), at the end of 2019:\n",
    "* [SageMaker Studio Announcement](https://aws.amazon.com/about-aws/whats-new/2019/12/introducing-amazon-sagemaker-studio-the-first-integrated-development-environment-ide-for-machine-learning/?trk=ls_card)\n",
    "* [SageMaker Experiments Announcement](https://aws.amazon.com/about-aws/whats-new/2019/12/introducing-amazon-sagemaker-experiments-organize-track-and-compare-your-machine-learning-training-experiments-on-amazon-sagemaker/?trk=ls_card)\n",
    "\n",
    "In this walkthrough, we will explore how Amazon SageMaker Studio, and the [Experiments SDK](https://sagemaker-experiments.readthedocs.io/en/latest/), which has been [open-sourced](https://github.com/aws/sagemaker-experiments), can be utilized to experiment with a Keras model and track data preprocessing required to prepare data for the model's consumption.\n",
    "\n",
    "Now, before we dive into hands-on exercise, let's first take a step back and discuss the building blocks of each Experiment and their referential relationships.\n",
    "* **Experiment** - a Machine Learning problem that we want to solve. Each experiment consists of a collection of Trials. Note that the name of an experiment must be unique in a given region of a particular AWS account.\n",
    "* **Trial** - an execution of a data-science workflow related to an experiment. Each Trial consists of several Trial Components. Note that the name of a trial must be unique in a given region of a particular AWS account.\n",
    "* **Trial Component** - a stage in a given trial. For instance, as we will see in our example, we will create one Trial Component for data pre-preprocessing stage and one Trial Component for model training. Similarly, in other use cases, we can also have a Trial Component for data post-processing. Unlike Experiments and Trials, Trial Components do not have to be uniquely named as they tend to represent the typical and very common stages in an ML pipeline.\n",
    "* **Tracker** - a mechanism that records various metadata about a particular Trial Component, including any Parameters, Inputs, Outputs, Artifacts and Metrics. When creating a Tracker, each Tracker is linked to a particular Training Component.\n",
    "\n",
    "![](img/experiment_structure_t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop, Train, Optimize and Deploy Scikit-Learn Random Forest\n",
    "\n",
    "> *This notebook should work well with the `Python 3 (Data Science)` kernel in SageMaker Studio, or the `conda_python3` kernel in SageMaker Notebook Instances*\n",
    "\n",
    "In this notebook we show how to use Amazon SageMaker to develop, train, tune and deploy a Random Forest model based using the popular ML framework [Scikit-Learn](https://scikit-learn.org/stable/index.html).\n",
    "\n",
    "The example uses the *Boston Housing dataset* (provided by Scikit-Learn) - more details of which can be found [here](https://scikit-learn.org/stable/datasets/index.html#boston-dataset).\n",
    "\n",
    "To understand the code, you might also find it useful to refer to:\n",
    "\n",
    "* The guide on [Using Scikit-Learn with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/using_sklearn.html)\n",
    "* The API doc for [Scikit-Learn classes in the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/sagemaker.sklearn.html)\n",
    "* The [SageMaker reference for Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#client) (The general AWS SDK for Python, including low-level bindings for SageMaker as well as many other AWS services)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting sagemaker-experiments\n",
      "  Downloading sagemaker_experiments-0.1.35-py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 99 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.7/site-packages (from sagemaker-experiments) (1.20.23)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.23.23)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.16.27->sagemaker-experiments) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.16.27->sagemaker-experiments) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.23->boto3>=1.16.27->sagemaker-experiments) (1.14.0)\n",
      "Installing collected packages: sagemaker-experiments\n",
      "Successfully installed sagemaker-experiments-0.1.35\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup libraries and environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bucket sagemaker-us-east-1-349934754982\n"
     ]
    }
   ],
   "source": [
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import tarfile\n",
    "\n",
    "# External Dependencies:\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_session.region_name\n",
    "bucket = sess.default_bucket()  # this could also be a hard-coded bucket name\n",
    "\n",
    "project_path = 'experiments-sklearn'\n",
    "processed_data_path = os.path.join('s3://',bucket,project_path,'data/processed/')\n",
    "artifacts_path = os.path.join(project_path,'artifacts')\n",
    "output_path = os.path.join('s3://',bucket,project_path,'output/')\n",
    "\n",
    "print('Using bucket ' + bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "We load a dataset from sklearn, split it and send it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the Boston housing dataset \n",
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, test_size=0.25, random_state=42)\n",
    "\n",
    "trainX = pd.DataFrame(X_train, columns=data.feature_names)\n",
    "trainX['target'] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "testX['target'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.09103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>7.155</td>\n",
       "      <td>92.2</td>\n",
       "      <td>2.7006</td>\n",
       "      <td>3.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>394.12</td>\n",
       "      <td>4.82</td>\n",
       "      <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.53501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>6.152</td>\n",
       "      <td>82.6</td>\n",
       "      <td>1.7455</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>88.01</td>\n",
       "      <td>15.02</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03578</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>7.820</td>\n",
       "      <td>64.5</td>\n",
       "      <td>4.6947</td>\n",
       "      <td>5.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>387.31</td>\n",
       "      <td>3.76</td>\n",
       "      <td>45.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.38735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>5.613</td>\n",
       "      <td>95.6</td>\n",
       "      <td>1.7572</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>359.29</td>\n",
       "      <td>27.26</td>\n",
       "      <td>15.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>6.333</td>\n",
       "      <td>17.2</td>\n",
       "      <td>5.2146</td>\n",
       "      <td>4.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>375.21</td>\n",
       "      <td>7.34</td>\n",
       "      <td>22.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS     NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.09103   0.0   2.46   0.0  0.4880  7.155  92.2  2.7006  3.0  193.0   \n",
       "1  3.53501   0.0  19.58   1.0  0.8710  6.152  82.6  1.7455  5.0  403.0   \n",
       "2  0.03578  20.0   3.33   0.0  0.4429  7.820  64.5  4.6947  5.0  216.0   \n",
       "3  0.38735   0.0  25.65   0.0  0.5810  5.613  95.6  1.7572  2.0  188.0   \n",
       "4  0.06724   0.0   3.24   0.0  0.4600  6.333  17.2  5.2146  4.0  430.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  \n",
       "0     17.8  394.12   4.82    37.9  \n",
       "1     14.7   88.01  15.02    15.6  \n",
       "2     14.9  387.31   3.76    45.4  \n",
       "3     19.1  359.29  27.26    15.7  \n",
       "4     16.9  375.21   7.34    22.6  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directories\n",
    "! mkdir -p data\n",
    "! mkdir -p source\n",
    "! mkdir -p model\n",
    "\n",
    "# save data as csv\n",
    "trainX.to_csv('data/boston_train.csv')\n",
    "testX.to_csv('data/boston_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training script\n",
    "\n",
    "The SageMaker Scikit-Learn Framework Container provides the basic runtime, and we as users specify the actual training steps to run as a script file (or even a folder of several, perhaps including a *requirements.txt* file).\n",
    "\n",
    "The below code initializes a `.py` file from here in the notebook.\n",
    "\n",
    "The same script can be used at training time (run as a script) and inference time (imported as a module) - So below we:\n",
    "\n",
    "- Define some specific functions to override default inference behavior (e.g. `model_fn()`), and\n",
    "- Enclose the training entry point in an `if __name__ == '__main__'` *guard clause* so it only executes when the module is run as a script.\n",
    "\n",
    "You can find detailed guidance in the documentation on [Preparing a Scikit-Learn training script](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#prepare-a-scikit-learn-training-script) (for training) and the [SageMaker Scikit-Learn model server](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#sagemaker-scikit-learn-model-server) (for inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing source/sklearn_training_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/sklearn_training_script.py\n",
    "# Python Built-Ins:\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# inference functions ---------------\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #------------------------------- parsing input parameters (from command line)\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # RandomForest hyperparameters\n",
    "    parser.add_argument('--n_estimators', type=int, default=10)\n",
    "    parser.add_argument('--min_samples_leaf', type=int, default=3)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train_dir', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test_dir', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train_file', type=str, default='boston_train.csv')\n",
    "    parser.add_argument('--test_file', type=str, default='boston_test.csv')\n",
    "    parser.add_argument('--features', type=str)  # explicitly name which features to use\n",
    "    parser.add_argument('--target_variable', type=str)  # explicitly name the column to be used as target\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    #------------------------------- data preparation\n",
    "    print('reading data')\n",
    "    train_df = pd.read_csv(os.path.join(args.train_dir, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test_dir, args.test_file))\n",
    "\n",
    "    print('building training and testing datasets')\n",
    "    X_train = train_df[args.features.split()]\n",
    "    X_test = test_df[args.features.split()]\n",
    "    y_train = train_df[args.target_variable]\n",
    "    y_test = test_df[args.target_variable]\n",
    "\n",
    "    #------------------------------- model training\n",
    "    print('training model')\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=args.n_estimators,\n",
    "        min_samples_leaf=args.min_samples_leaf,\n",
    "        n_jobs=-1)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #-------------------------------  model testing\n",
    "    print('testing model')\n",
    "    abs_err = np.abs(model.predict(X_test) - y_test)\n",
    "\n",
    "    # percentile absolute errors\n",
    "    for q in [10, 50, 90]:\n",
    "        print('AE-at-' + str(q) + 'th-percentile: '\n",
    "              + str(np.percentile(a=abs_err, q=q)))\n",
    "\n",
    "    #------------------------------- save model\n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print('model saved at ' + path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local training\n",
    "Script arguments allows us to remove from the script any SageMaker-specific configuration, and run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python source/sklearn_training_script.py \\\n",
    "    --n_estimators 100 \\\n",
    "    --min_samples_leaf 3 \\\n",
    "    --model_dir 'model/' \\\n",
    "    --train_dir 'data/' \\\n",
    "    --test_dir 'data/' \\\n",
    "    --train_file 'boston_train.csv' \\\n",
    "    --test_file 'boston_test.csv' \\\n",
    "    --features 'CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT' \\\n",
    "    --target_variable 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data input channels (copy to S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set URI: s3://sagemaker-us-east-1-349934754982/sm101/sklearn/boston_train.csv\n",
      "Test set URI: s3://sagemaker-us-east-1-349934754982/sm101/sklearn/boston_test.csv\n"
     ]
    }
   ],
   "source": [
    "# send data to S3. SageMaker will take training data from s3\n",
    "train_path_s3 = sess.upload_data(\n",
    "    path='data/boston_train.csv',  # source\n",
    "    bucket=bucket,\n",
    "    key_prefix='sm101/sklearn'  # destination path in S3\n",
    ")\n",
    "\n",
    "test_path_s3 = sess.upload_data(\n",
    "    path='data/boston_test.csv',  # source\n",
    "    bucket=bucket,\n",
    "    key_prefix='sm101/sklearn'  # destination path in S3\n",
    ")\n",
    "\n",
    "print('Train set URI:', train_path_s3)\n",
    "print('Test set URI:', test_path_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's utilize the Experiments SDK to create a Pre-processing Trial Component and keep track of what've accomplished so far. We start by creating an experiment. Remember that each experiment name must be unique within a given region of a particular AWS account.\n",
    "\n",
    "Once we have our experiment instantiated, we will proceed with creating a Tracker to capture various details about the data pre-processing we completed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "sm = boto3.client('sagemaker')\n",
    "ts = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "\n",
    "boston_experiment = Experiment.create(\n",
    "    experiment_name = 'predict-boston-' + ts,\n",
    "    description = 'Predicting target var from boston dataset',\n",
    "    sagemaker_boto_client=sm)\n",
    "\n",
    "\n",
    "with Tracker.create(display_name='Pre-processing', sagemaker_boto_client=sm, artifact_bucket=bucket, artifact_prefix=artifacts_path) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        'train_test_split': 0.75\n",
    "    })\n",
    "    tracker.log_input(name='raw data', media_type='s3/uri', value='sklearn.load_boston()')\n",
    "    tracker.log_output(name='train data', media_type='s3/uri', value=train_path_s3)\n",
    "    tracker.log_output(name='test data', media_type='s3/uri', value=test_path_s3)\n",
    "    \n",
    "processing_component = tracker.trial_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! We now have our experiment ready and we've already done our due diligence to capture our data pre-processing approach as a Trial Component. Next, let's dive into the modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with SageMaker Experiments\n",
    "We are ready to define a few sets of hyperparameters that we want to experiment with and kick off our training jobs to run in parallel. Note that we can also easily capture custom metrics during training by configuring a regex pattern to match the desired values in the logs generated by the training job. In our case, for demonstration purposes, we will capture the median absolute error (`median-AE`) that the sklearn framework reports as the metric for our model.\n",
    "\n",
    "Note that, in our example, we do not make any changes to our pre-processing strategy between different trials, so we will attach the same Pre-processing Trial Component that we create earlier, to each of the Trials we create in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sets of hyperparameters, create a trial for each and kick of training jobs\n",
    "hyperparameters_groups=[{\n",
    "                        'n_estimators': 50,\n",
    "                        'min_samples_leaf': 3,\n",
    "                        'features': 'CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT',\n",
    "                        'target_variable': 'target',\n",
    "                        },\n",
    "                        {\n",
    "                        'n_estimators': 100,\n",
    "                        'min_samples_leaf': 3,\n",
    "                        'features': 'CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT',\n",
    "                        'target_variable': 'target',\n",
    "                        },\n",
    "                        {\n",
    "                        'n_estimators': 50,\n",
    "                        'min_samples_leaf': 5,\n",
    "                        'features': 'CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT',\n",
    "                        'target_variable': 'target',\n",
    "                        },\n",
    "                        {\n",
    "                        'n_estimators': 100,\n",
    "                        'min_samples_leaf': 5,\n",
    "                        'features': 'CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT',\n",
    "                        'target_variable': 'target',\n",
    "                        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: boston-train-0-2022-03-01-17-28-22-900018\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: boston-train-1-2022-03-01-17-28-23-565612\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: boston-train-2-2022-03-01-17-28-26-715623\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: boston-train-3-2022-03-01-17-28-28-424670\n"
     ]
    }
   ],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "for i,hp_set in enumerate(hyperparameters_groups):\n",
    "\n",
    "    ts = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "    boston_trial = boston_experiment.create_trial(trial_name='boston-trial-' + str(i) + '-'+ ts)\n",
    "    boston_trial.add_trial_component(processing_component)\n",
    "    \n",
    "    boston_estimator = SKLearn(entry_point='source/sklearn_training_script.py',\n",
    "                                role=get_execution_role(),\n",
    "                                instance_count=1,\n",
    "                                instance_type='ml.m5.large',\n",
    "                                framework_version='0.23-1',\n",
    "                                base_job_name='rf-scikit',\n",
    "                                hyperparameters=hp_set,\n",
    "                                output_path = output_path,\n",
    "                                metric_definitions=[\n",
    "                                    { 'Name': 'median-AE', 'Regex': 'AE-at-50th-percentile: ([0-9.]+).*$' },\n",
    "                                ],\n",
    "                                max_run=20*60,  # Maximum allowed active runtime (in seconds)\n",
    "                                use_spot_instances=True,  # Use spot instances to reduce cost\n",
    "                                max_wait=30*60,  # Maximum clock time (including spot delays)\n",
    "                            )\n",
    "    \n",
    "\n",
    "\n",
    "    job_name = 'boston-train-' + str(i) + '-'+ ts\n",
    "    \n",
    "    boston_estimator.fit({'train':train_path_s3, 'test': test_path_s3},\n",
    "                          job_name=job_name,\n",
    "                          wait=False,\n",
    "                          experiment_config={\n",
    "                                            'ExperimentName': boston_experiment.experiment_name,\n",
    "                                            'TrialName': boston_trial.trial_name,\n",
    "                                            'TrialComponentDisplayName': 'Training',\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the training job that we ran is very \"light\", due to the very small dataset. As such, running locally on the notebook instance results in a faster execution time, compared to SageMaker. SageMaker takes longer time to run the job because it has to provision the training infrastructure. Since this example training job not very resource-intensive, the infrastructure provisioning process adds more overhead, compared to the training job itself. \n",
    "\n",
    "In a real situation, where datasets are large, running on SageMaker can considerably speed up the execution process - and help us optimize costs, by keeping this interactive notebook environment modest and spinning up more powerful training job resources on-demand.\n",
    "\n",
    "Note that this training job *did not run here on the notebook itself*. You'll be able to see the history in the [AWS Console for SageMaker - Training Jobs tab](https://console.aws.amazon.com/sagemaker/home?#/jobs).\n",
    "\n",
    "> ℹ️ **Tip:** There's **no need to re-run** a training job if your notebook kernel restarts or the estimator state is lost for some other reason... You can just *attach* to a previous training job by name - for example:\n",
    ">\n",
    "> ```python\n",
    "> estimator = SKLearn.attach('rf-scikit-2025-01-01-00-00-00-000')\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Upon completion of the training jobs, we can, in just a few seconds, start visualizing how different variations of the model compare in terms of the metrics collected during model training. For instance, in just a few clicks, we can visualize how the loss has been decreasing by epoch for each variation of the model and very quickly observe the model architecture that is most effective in decreasing the loss.\n",
    "\n",
    "![](img/visualize_loss.png)\n",
    "\n",
    "You can create the same plot by following the five simple steps listed below:\n",
    "1. Select the *SageMaker Experiments List* icon on the left sidebar.\n",
    "2. Double-click on your experiment to open it and use Shift key on your keyboard to select all four trials.\n",
    "3. Right-click on any of the highlighted trials and select *Open in trial component list*.\n",
    "4. Use Shift key on your keyboard to select the four Trial Components representing the Training jobs and click on *Add chart* button.\n",
    "5. Click on *New chart* and customize it to plot the collected metrics that you would like to analyze.\n",
    "\n",
    "![](img/visualization_steps_line.gif)\n",
    "\n",
    "Similarly, we can generate a scatter plot that helps us determine whether there is a relationship between the size of the first hidden layer in the network and the Mean Squared Logarithmic Error.\n",
    "\n",
    "![](img/scatter_msle.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
